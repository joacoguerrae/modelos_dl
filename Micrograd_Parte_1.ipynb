{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduccion a redes neuronales y backpropagation - Micrograd\n",
        "\n",
        "En esta notebook, exploraremos los conceptos fundamentales detr谩s del c谩lculo autom谩tico de gradientes, un componente esencial para entrenamiento de redes neuronales. Nos basaremos en la primera lecci贸n del curso [Neural Networks: Zero to Hero](https://github.com/karpathy/nn-zero-to-hero/tree/master) de Andrej Karpathy y su proyecto [Micrograd](https://github.com/karpathy/micrograd), una implementaci贸n simplificada de un motor de autogradiente.\n",
        "\n",
        "## Objetivos\n",
        "*   Comenzaremos revisando la definici贸n de la derivada de una funci贸n y c贸mo calcularla num茅ricamente.\n",
        "*   Introduciremos la clase `Value`, que representa un escalar y almacena informaci贸n crucial para backpropagation.\n",
        "*   A trav茅s de ejemplos y visualizaciones, construiremos el Directed Acyclic Graph (DAG) que representa las operaciones.\n",
        "*   Finalmente, implementaremos el algoritmo de backpropagation.\n",
        "\n",
        "Al final de esta notebook, tendr谩s una comprensi贸n s贸lida de c贸mo funciona el c谩lculo de gradientes y su importancia en la optimizaci贸n de modelos."
      ],
      "metadata": {
        "id": "UlxKL_Q3IW4h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iq2ywOwkIP-f"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Derivada de una funci贸n\n",
        "\n",
        "La derivada de una funci贸n se calcula como\n",
        "\n",
        "$$\n",
        "f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}\n",
        "$$\n",
        "\n",
        "Tenemos una funci贸n $f(x) = 3x^2 - 4x + 5$\n",
        "\n",
        "Podemos calcular su derivada en el punto $x=3$ como\n",
        "\n",
        "$$\n",
        "f'(3) = \\lim_{h \\to 0} \\frac{f(3+h) - f(3)}{h}\n",
        "$$"
      ],
      "metadata": {
        "id": "t8d99XI1MlAA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGa1MBOGIP-g"
      },
      "outputs": [],
      "source": [
        "def f(x):\n",
        "  return 3*x**2 - 4*x + 5\n",
        "\n",
        "xs = np.arange(-5, 5, 0.25)\n",
        "ys = f(xs)\n",
        "plt.plot(xs, ys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLrwYlF6IP-i"
      },
      "outputs": [],
      "source": [
        "# Si h es demasiado cercano a 0, por la limitaci贸n de representaci贸n de floating point\n",
        "# el resultado nos va a dar 0, lo que es incorrecto.\n",
        "h = 0.000001\n",
        "x = 3\n",
        "(f(x + h) - f(x))/h"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = 3\n",
        "y0 = f(3)\n",
        "m = (f(x0 + h) - f(x0)) / h  # pendiente en 3\n",
        "\n",
        "def tangent_line(x):\n",
        "    return m * (x - x0) + y0\n",
        "\n",
        "# Rango solo para la tangente\n",
        "xs_tan = np.linspace(1, 4.5, 100)\n",
        "\n",
        "plt.plot(xs, ys, label=\"f(x)\")\n",
        "plt.plot(xs_tan, tangent_line(xs_tan), '--', label=f\"Tangente en x={x0}\")\n",
        "plt.scatter([x0], [y0], color=\"red\")  # punto de tangencia\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PkRhC1gL8-1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Derivada de una funci贸n con m谩s de un valor\n",
        "\n",
        "Si queremos calcular la derivada de la funci贸n $d(a,b,c) = a \\cdot b + c$\n",
        "\n",
        "Tenemos que decidir respecto a que variable vamos a derivar, por ejemplo, si derivamos respecto a $a$, tenemos\n",
        "\n",
        "$$\n",
        "\\frac{\\partial d}{\\partial a} = \\lim_{h \\to 0} \\frac{d(a+h,b,c) - d(a,b,c)}{h}\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "R7rYMbGwOVPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def d(a, b, c):\n",
        "  return a*b + c\n",
        "\n",
        "a = 2.0\n",
        "b = -3.0\n",
        "c = 10.0\n",
        "d(a, b, c)"
      ],
      "metadata": {
        "id": "hkSxB7e7PZUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0dpa5wUIP-i"
      },
      "outputs": [],
      "source": [
        "h = 0.0001\n",
        "\n",
        "# inputs\n",
        "a = 2.0\n",
        "b = -3.0\n",
        "c = 10.0\n",
        "\n",
        "print(\"Derivada con respecto a 'a'\")\n",
        "d1 = d(a,b,c)\n",
        "d2 = d(a+h,b,c)\n",
        "\n",
        "print('d(a, b, c)', d1)\n",
        "print('d(a, b+h, c)', d2)\n",
        "print('slope', (d2 - d1)/h)\n",
        "\n",
        "print(\"\\nDerivada con respecto a 'b'\")\n",
        "d1 = d(a,b,c)\n",
        "d2 = d(a,b+h,c)\n",
        "\n",
        "print('d(a, b, c)', d1)\n",
        "print('d(a, b+h, c)', d2)\n",
        "print('slope', (d2 - d1)/h)\n",
        "\n",
        "print(\"\\nDerivada con respecto a 'c'\")\n",
        "d1 = d(a,b,c)\n",
        "d2 = d(a,b,c+h)\n",
        "\n",
        "print('d(a, b, c)', d1)\n",
        "print('d(a, b, c+h)', d2)\n",
        "print('slope', (d2 - d1)/h)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Construyendo el n煤cleo de Micrograd\n",
        "\n",
        "Vamos a crear una clase que contendr谩 el **valor num茅rico** asociado a una variable, junto con otros datos 煤tiles que necesitaremos para realizar **Backpropagation**.  \n",
        "\n",
        "La clase **`Value`** en *Micrograd* no es m谩s que una versi贸n simplificada de la clase **`Tensor`** de [PyTorch](https://pytorch.org/).  \n",
        "La diferencia principal es que:  \n",
        "- En **`Value`** solo trabajaremos con **escalares**.  \n",
        "- En **PyTorch**, en cambio, los tensores pueden tener cualquier dimensi贸n y soportar operaciones mucho m谩s complejas.  \n",
        "\n",
        "De esta manera, Micrograd nos permite enfocarnos en lo esencial de c贸mo funciona el c谩lculo autom谩tico de gradientes, sin la complejidad adicional de trabajar con tensores de mayor dimensi贸n.\n"
      ],
      "metadata": {
        "id": "lzApv3QZRDPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Value:\n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "\n",
        "  def __repr__(self): # Esta funci贸n nos permite imprimir el Value de una forma legible\n",
        "    return f\"Value(data={self.data})\"\n",
        "\n",
        "  def __add__(self, other): # La operaci贸n suma: Value + Value\n",
        "    out = Value(self.data + other.data)\n",
        "    return out\n",
        "\n",
        "  def __mul__(self, other): # La operaci贸n multiplicaci贸n: Value * Value\n",
        "    out = Value(self.data * other.data)\n",
        "    return out"
      ],
      "metadata": {
        "id": "JtDhvHdNRtA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = Value(2.0)\n",
        "b = Value(-3.0)\n",
        "c = Value(10.0)\n",
        "d = a*b + c\n",
        "d"
      ],
      "metadata": {
        "id": "4cGmA7jESwXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Creando el Directed Acyclic Graph (DAG)\n",
        "\n",
        "La clase **`Value`** nos da una estructura b谩sica, pero lo que realmente necesitamos es una forma de **relacionar los valores entre s铆**. Queremos poder reconstruir el camino de operaciones que llevaron a cada `Value`, y para eso vamos a usar un **grafo dirigido ac铆clico (DAG)**.  \n",
        "\n",
        "En este grafo, cada `Value` conoce:  \n",
        "- **Los valores de los que proviene** (sus *padres* o *hijos*, seg煤n c贸mo se mire).  \n",
        "- **La operaci贸n** que lo gener贸.  \n",
        "- **El gradiente** asociado, que por ahora inicializamos en `0`.  \n",
        "\n",
        "De esta manera, no solo tenemos los resultados de las operaciones, sino tambi茅n toda la informaci贸n necesaria para aplicar **backpropagation** m谩s adelante.\n"
      ],
      "metadata": {
        "id": "c7gZMrKZS1DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nueva implementaci贸n de la clase value\n",
        "class Value:\n",
        "  def __init__(self, data, _children=(), _op='', label=''): # Value ahora recibe sus 'hijos' y la operaci贸n que se realiz贸\n",
        "    self.data = data\n",
        "    self.grad = 0 # Gradiente inicializado en 0\n",
        "    self._prev = set(_children)\n",
        "    self._op = _op\n",
        "    self.label = label # label nos mejora la visualizaci贸n de los datos\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"Value(data={self.data})\"\n",
        "\n",
        "  def __add__(self, other):\n",
        "    out = Value(self.data + other.data, (self,other), '+') # Guardamos en el nuevo value, los values utilizados y la operaci贸n '+'\n",
        "    return out\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    out = Value(self.data * other.data, (self, other), '*') # Guardamos en el nuevo value, los values utilizados y la operaci贸n '*'\n",
        "    return out"
      ],
      "metadata": {
        "id": "A7GUM3pfTKUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizaci贸n**\n",
        "\n",
        "Para visualizar estas nuevas relaciones, vamos a utilizar funciones auxiliares que utilizan [Graphviz](https://graphviz.org/) para crear visualizaciones"
      ],
      "metadata": {
        "id": "B-aIqjN-TvM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from graphviz import Digraph\n",
        "\n",
        "def trace(root):\n",
        "  # builds a set of all nodes and edges in a graph\n",
        "  nodes, edges = set(), set()\n",
        "  def build(v):\n",
        "    if v not in nodes:\n",
        "      nodes.add(v)\n",
        "      for child in v._prev:\n",
        "        edges.add((child, v))\n",
        "        build(child)\n",
        "  build(root)\n",
        "  return nodes, edges\n",
        "\n",
        "def draw_dot(root):\n",
        "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
        "\n",
        "  nodes, edges = trace(root)\n",
        "  for n in nodes:\n",
        "    uid = str(id(n))\n",
        "    # for any value in the graph, create a rectangular ('record') node for it\n",
        "    dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
        "    if n._op:\n",
        "      # if this value is a result of some operation, create an op node for it\n",
        "      dot.node(name = uid + n._op, label = n._op)\n",
        "      # and connect this node to it\n",
        "      dot.edge(uid + n._op, uid)\n",
        "\n",
        "  for n1, n2 in edges:\n",
        "    # connect n1 to the op node of n2\n",
        "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
        "\n",
        "  return dot\n"
      ],
      "metadata": {
        "id": "6yONlJhtULmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = Value(2.0, label='a')\n",
        "b = Value(-3.0, label='b')\n",
        "c = Value(10.0, label='c')\n",
        "e = a*b; e.label = 'e' # Agregamos e para tener una Label en la operaci贸n a*b\n",
        "d = e + c; d.label = 'd'\n",
        "f = Value(-2.0, label='f')\n",
        "L = d * f; L.label = 'L'\n",
        "L\n",
        "draw_dot(L)"
      ],
      "metadata": {
        "id": "m-QPa7W0UPB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Gradiente\n",
        "\n",
        "驴Qu茅 significa el gradiente en nuestro contexto?  \n",
        "El gradiente de cada nodo `Value` representa la **derivada de ese valor respecto al nodo final** (en este caso, *L*).\n",
        "\n",
        "- Para el nodo final (*L*), el gradiente siempre es `1.0`, porque:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial L} = 1\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Gradiente en la multiplicaci贸n\n",
        "\n",
        "Tenemos que:\n",
        "\n",
        "$$\n",
        "L = d \\cdot f\n",
        "$$\n",
        "\n",
        "y queremos calcular:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial d}\n",
        "$$\n",
        "\n",
        "Usamos la definici贸n de derivada como l铆mite:\n",
        "\n",
        "$$\n",
        "\\frac{(d+h) \\cdot f - d \\cdot f}{h} =\n",
        "\\frac{d \\cdot f + h \\cdot f - d \\cdot f}{h} =\n",
        "\\frac{h \\cdot f}{h} = f\n",
        "$$\n",
        "\n",
        "Entonces:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial d} = f\n",
        "$$\n",
        "\n",
        "De manera sim茅trica:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial f} = d\n",
        "$$\n",
        "\n",
        "Esto nos da una **regla general para la multiplicaci贸n**:  \n",
        "- El gradiente con respecto a un factor es el **otro factor**.\n",
        "\n",
        "### Aplicado a nuestro ejemplo\n",
        "\n",
        "Si tenemos $d = 4$ y $f = -2$, entonces:  \n",
        "- El gradiente de $d$ es $f = -2$.  \n",
        "- El gradiente de $f$ es $d = 4$.  \n"
      ],
      "metadata": {
        "id": "4IHnanamWq_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## La regla de la cadena\n",
        "\n",
        "Ahora nos preguntamos: 驴c贸mo calculamos los gradientes del **resto de los valores** con respecto a $L$?  \n",
        "\n",
        "Tomemos el caso de **$c$**. Queremos calcular:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial c}\n",
        "$$\n",
        "\n",
        "El problema es que $c$ no est谩 conectado directamente con $L$ en el grafo, sino a trav茅s de $d$.  \n",
        "驴C贸mo podemos entonces calcular $\\frac{\\partial L}{\\partial c}$?  \n",
        "Aplicando la **regla de la cadena**.\n",
        "\n",
        "En t茅rminos generales:\n",
        "\n",
        "$$\n",
        "h'(x) = f'(g(x)) \\cdot g'(x)\n",
        "$$\n",
        "\n",
        "O, dicho de otra forma: si una variable $z$ depende de $y$ y $y$ depende de $x$, entonces:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Aplicado a nuestro ejemplo\n",
        "\n",
        "En nuestro grafo, $L$ depende de $c$ **a trav茅s de $d$**.  \n",
        "Podemos expresarlo como:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial c} = \\frac{\\partial L}{\\partial d} \\cdot \\frac{\\partial d}{\\partial c}\n",
        "$$\n",
        "\n",
        "Ya conocemos $\\frac{\\partial L}{\\partial d}$ (lo guardamos en el gradiente de $d$).  \n",
        "Lo que falta calcular es $\\frac{\\partial d}{\\partial c}$, a lo que llamamos **gradiente local**.\n",
        "\n",
        "---\n",
        "\n",
        "### Gradiente en la suma\n",
        "\n",
        "Sabemos que:\n",
        "\n",
        "$$\n",
        "d = c + e\n",
        "$$\n",
        "\n",
        "Entonces, derivando respecto a $c$:\n",
        "\n",
        "$$\n",
        "\\frac{(c+h)+e - (c+e)}{h} =\n",
        "\\frac{c+h+e - c - e}{h} =\n",
        "\\frac{h}{h} = 1.0\n",
        "$$\n",
        "\n",
        "Por lo tanto:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial d}{\\partial c} = 1.0\n",
        "$$\n",
        "\n",
        "Y de manera an谩loga:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial d}{\\partial e} = 1.0\n",
        "$$\n"
      ],
      "metadata": {
        "id": "DZGWzGYqa8Hn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "\n",
        "## Terminando el ejemplo\n",
        "\n",
        "Ya tenemos:\n",
        "\n",
        "- $\\frac{\\partial L}{\\partial d} = -2$\n",
        "- $\\frac{\\partial d}{\\partial c} = 1$\n",
        "- $\\frac{\\partial d}{\\partial e} = 1$\n",
        "\n",
        "Con esto, podemos calcular el gradiente de $c$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial c} =\n",
        "\\frac{\\partial L}{\\partial d} \\cdot \\frac{\\partial d}{\\partial c} =\n",
        "-2 \\cdot 1 = -2\n",
        "$$\n",
        "\n",
        "En el caso de la **suma**, el gradiente simplemente **se copia a los operandos**.  \n",
        "Por lo tanto, el gradiente de $e$ tambi茅n es:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial e} =\n",
        "\\frac{\\partial L}{\\partial d} \\cdot \\frac{\\partial d}{\\partial e} =\n",
        "-2 \\cdot 1 = -2\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Gradientes de $a$ y $b$\n",
        "\n",
        "Ahora nos faltan los gradientes de $a$ y $b$.  \n",
        "\n",
        "Sabemos que:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial a} =\n",
        "\\frac{\\partial L}{\\partial d} \\cdot\n",
        "\\frac{\\partial d}{\\partial e} \\cdot\n",
        "\\frac{\\partial e}{\\partial a}\n",
        "$$\n",
        "\n",
        "Pero ya vimos que:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial e} =\n",
        "\\frac{\\partial L}{\\partial d} \\cdot \\frac{\\partial d}{\\partial e} = -2\n",
        "$$\n",
        "\n",
        "As铆 que solo queda calcular $\\frac{\\partial e}{\\partial a}$ (y an谩logamente para $b$).  \n",
        "Como $e = a \\cdot b$, aplicamos la regla de la multiplicaci贸n:\n",
        "\n",
        "- $\\frac{\\partial e}{\\partial a} = b = -3$\n",
        "- $\\frac{\\partial e}{\\partial b} = a = 2$\n",
        "\n",
        "---\n",
        "\n",
        "### Resultado final\n",
        "\n",
        "- Gradiente de $a$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial a} =\n",
        "\\frac{\\partial e}{\\partial a} \\cdot \\frac{\\partial L}{\\partial e} =\n",
        "(-3) \\cdot (-2) = 6\n",
        "$$\n",
        "\n",
        "- Gradiente de $b$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b} =\n",
        "\\frac{\\partial e}{\\partial b} \\cdot \\frac{\\partial L}{\\partial e} =\n",
        "(2) \\cdot (-2) = -4\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "En resumen:  \n",
        "- El gradiente se **propaga hacia atr谩s** siguiendo las conexiones del grafo.  \n",
        "- En las sumas, el gradiente se copia.  \n",
        "- En las multiplicaciones, el gradiente se multiplica por el otro factor.  \n"
      ],
      "metadata": {
        "id": "Pz6blvgxfWu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pongamos los valores calculados en nuestro grafo para visualizarlos\n",
        "a.grad = 6\n",
        "b.grad = -4\n",
        "c.grad = -2\n",
        "e.grad = -2\n",
        "d.grad = -2\n",
        "f.grad = 4\n",
        "L.grad = 1\n",
        "draw_dot(L)"
      ],
      "metadata": {
        "id": "aCPW8qW6j99Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 驴Para qu茅 sirve calcular los gradientes?\n",
        "\n",
        "Teniendo los gradientes de cada variable respecto a $L$, podemos interpretar **c贸mo cambiar铆a $L$ si modificamos esas variables**.  \n",
        "Por ejemplo, si el gradiente de una variable es positivo, aumentar ligeramente su valor har谩 que $L$ tambi茅n aumente; si es negativo, al aumentarla, $L$ disminuir谩.\n",
        "\n",
        "Este proceso es la base de c贸mo funcionan los algoritmos de **optimizaci贸n** (como *gradient descent*): ajustar las variables en la direcci贸n que reduce la funci贸n de p茅rdida.\n",
        "\n",
        "锔 Importante: este ajuste solo lo hacemos sobre los **nodos hoja**, es decir, aquellos que no dependen de otros y que representan las variables independientes de nuestro modelo.\n"
      ],
      "metadata": {
        "id": "ctrjsFrq8Vbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a.data += 0.01 * a.grad\n",
        "b.data += 0.01 * b.grad\n",
        "c.data += 0.01 * c.grad\n",
        "f.data += 0.01 * f.grad\n",
        "\n",
        "# Volvemos a calcular\n",
        "e = a*b\n",
        "d = e + c\n",
        "L = d * f\n",
        "L"
      ],
      "metadata": {
        "id": "N0eP5PpslCYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Backpropagation\n",
        "\n",
        "El proceso que acabamos de realizar es lo que llamamos **Backpropagation**.\n",
        "\n",
        "Obviamente, calcular Backpropagation manualmente para c谩da nodo es ridiculo asi que vamos a ponerle un fin al sufrimiento y automatizemos el proceso."
      ],
      "metadata": {
        "id": "ownwkOxV_Kda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nueva implementaci贸n de la clase value\n",
        "class Value:\n",
        "  def __init__(self, data, _children=(), _op='', label=''):\n",
        "    self.data = data\n",
        "    self.grad = 0\n",
        "    self._backward = lambda: None # Funci贸n que va a propagar el gradiente en cada nodo, por defecto no hace nada\n",
        "    self._prev = set(_children)\n",
        "    self._op = _op\n",
        "    self.label = label\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"Value(data={self.data},label={self.label})\" # Agregamos label para mejorar visualizaci贸n\n",
        "\n",
        "  def __add__(self, other):\n",
        "    out = Value(self.data + other.data, (self,other), '+')\n",
        "\n",
        "    def _backward(): # Clausura\n",
        "      self.grad = 1.0 * out.grad\n",
        "      other.grad = 1.0 * out.grad\n",
        "\n",
        "    out._backward = _backward # Guardamos la funci贸n backward para el nodo suma\n",
        "    return out\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    out = Value(self.data * other.data, (self, other), '*')\n",
        "\n",
        "    def _backward(): # Clausura\n",
        "      self.grad = other.data * out.grad\n",
        "      other.grad = self.data * out.grad\n",
        "\n",
        "    out._backward = _backward # Guardamos la funci贸n backward para el nodo multiplicaci贸n\n",
        "    return out"
      ],
      "metadata": {
        "id": "HQG8OuAXAP7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = Value(2.0, label='a')\n",
        "b = Value(-3.0, label='b')\n",
        "c = Value(10.0, label='c')\n",
        "e = a*b; e.label = 'e'\n",
        "d = e + c; d.label = 'd'\n",
        "f = Value(-2.0, label='f')\n",
        "L = d * f; L.label = 'L'\n",
        "L\n",
        "draw_dot(L)"
      ],
      "metadata": {
        "id": "4LdxBLbkCNo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora podemos autom谩ticamente propagar el gradiente por **un nodo** llamando a su funci贸n `backward()`"
      ],
      "metadata": {
        "id": "xHccdrH8Cygt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cuidado! Backward propaga el gradiente a los inputs utilizando el del output, pero al comenzar el gradiente es 0\n",
        "L.grad = 1.0 # Por tanto, necesitamos setear el gradiente de L = 1\n",
        "L._backward()\n",
        "draw_dot(L)"
      ],
      "metadata": {
        "id": "ROazLxlsCuPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora podemos continuar llamando el backward de todos los nodos, en orden\n",
        "d._backward()\n",
        "e._backward()\n",
        "draw_dot(L)"
      ],
      "metadata": {
        "id": "8KK5JJOBDgq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extra:** Que pasa si llamamos a `_backward()` de un nodo 'hoja'?"
      ],
      "metadata": {
        "id": "3QwB5-FDD2ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f._backward()"
      ],
      "metadata": {
        "id": "FhkZiUR3Dy0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Implementando `backward` para todo el grafo\n",
        "\n",
        "La 煤ltima pieza que nos falta es poder calcular **todos los gradientes** con una sola llamada a `.backward()`.  \n",
        "\n",
        "La idea es la siguiente: una vez que tenemos la expresi贸n completa en nuestro grafo, queremos recorrerla **de atr谩s hacia adelante**, es decir, desde el nodo final (*L*) hasta los nodos hoja.  \n",
        "De esta forma podemos propagar los gradientes a lo largo de todas las operaciones que construyeron el resultado.\n",
        "\n",
        "Para lograrlo, necesitamos asegurarnos de visitar los nodos en un **orden correcto**, donde cada nodo se procese *despu茅s* de todos los nodos de los que depende.  \n",
        "\n",
        "El algoritmo que nos resuelve este problema se llama  \n",
        " [**Ordenamiento Topol贸gico (Topological Sort)**](https://es.wikipedia.org/wiki/Ordenamiento_topol%C3%B3gico).  \n",
        "\n",
        "Con este algoritmo, podemos recorrer el grafo en el orden adecuado y aplicar backpropagation autom谩ticamente para cada nodo.\n",
        "\n",
        "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmiro.medium.com%2Fmax%2F1400%2F1*uMg_ojFXts2WZSjcZe4oRQ.png&f=1&nofb=1&ipt=24e248e39237c41398f3f145394cbb5db0ee64a7b00c9ff7ef70a10028b8829a\" width=\"700\">"
      ],
      "metadata": {
        "id": "TRIsEbj2EUFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = Value(2.0, label='a')\n",
        "b = Value(-3.0, label='b')\n",
        "c = Value(10.0, label='c')\n",
        "e = a*b; e.label = 'e'\n",
        "d = e + c; d.label = 'd'\n",
        "f = Value(-2.0, label='f')\n",
        "L = d * f; L.label = 'L'\n",
        "L\n",
        "draw_dot(L)"
      ],
      "metadata": {
        "id": "C7a9FylUPGbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topo = []\n",
        "visited = set()\n",
        "\n",
        "def build_topo(v):\n",
        "  if v not in visited:\n",
        "    visited.add(v)\n",
        "    for child in v._prev:\n",
        "      build_topo(child) # Continuamos hacia sus inputs\n",
        "    topo.append(v)\n",
        "\n",
        "build_topo(L)\n",
        "topo"
      ],
      "metadata": {
        "id": "u7NyCM3pN4q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L.grad = 1\n",
        "for node in reversed(topo):\n",
        "  node._backward()"
      ],
      "metadata": {
        "id": "ybeJCE3uK539"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw_dot(L)"
      ],
      "metadata": {
        "id": "IqtFLPa1HsPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Por 煤ltimo, lo que vamos a hacer es mover esta l贸gica a una funci贸n dentro de nuestra clase Value para poder llamarla c贸modamente."
      ],
      "metadata": {
        "id": "wn29qy2eRtwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nueva implementaci贸n de la clase value\n",
        "class Value:\n",
        "  def __init__(self, data, _children=(), _op='', label=''):\n",
        "    self.data = data\n",
        "    self.grad = 0\n",
        "    self._backward = lambda: None\n",
        "    self._prev = set(_children)\n",
        "    self._op = _op\n",
        "    self.label = label\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"Value(data={self.data},label={self.label})\"\n",
        "\n",
        "  def __add__(self, other):\n",
        "    out = Value(self.data + other.data, (self,other), '+')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad = 1.0 * out.grad\n",
        "      other.grad = 1.0 * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    out = Value(self.data * other.data, (self, other), '*')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad = other.data * out.grad\n",
        "      other.grad = self.data * out.grad\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def backward(self): # Funci贸n backward integrada en la clase value\n",
        "    topo = []\n",
        "    visited = set()\n",
        "\n",
        "    def build_topo(v):\n",
        "      if v not in visited:\n",
        "        visited.add(v)\n",
        "        for child in v._prev:\n",
        "          build_topo(child) # Continuamos hacia sus inputs\n",
        "        topo.append(v)\n",
        "\n",
        "    build_topo(self)\n",
        "\n",
        "    self.grad = 1\n",
        "    for node in reversed(topo):\n",
        "      node._backward()"
      ],
      "metadata": {
        "id": "NTDR2BEyRgcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = Value(2.0, label='a')\n",
        "b = Value(-3.0, label='b')\n",
        "c = Value(10.0, label='c')\n",
        "e = a*b; e.label = 'e'\n",
        "d = e + c; d.label = 'd'\n",
        "f = Value(-2.0, label='f')\n",
        "L = d * f; L.label = 'L'\n",
        "L\n",
        "draw_dot(L)"
      ],
      "metadata": {
        "id": "Yjp-9AVKR-w6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L.backward()\n",
        "draw_dot(L)"
      ],
      "metadata": {
        "id": "OeMrwSZOR_tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Arreglando un peque帽o bug de backprop\n",
        "\n",
        "Nuestro c贸digo tiene un bug que ocurre solo en casos espec铆ficos, cuando un nodo es utilizado m谩s de una vez en nuestro grafo"
      ],
      "metadata": {
        "id": "J4Bh8foJSObN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = Value(3.0, label='a')\n",
        "b = a+a; b.label = 'b'\n",
        "b.backward()\n",
        "draw_dot(b)"
      ],
      "metadata": {
        "id": "pDHJ8hi3ShED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver que el gradiente de `a` (la derivada de $b$ respecto a `a`) fue calculado como `1` por nuestro algoritmo de backpropagation.  \n",
        "Sin embargo, la derivada correcta deber铆a ser `2`.\n",
        "\n",
        "**驴Por que ocurre esto?**\n",
        "\n",
        "Si miramos nuestro c贸digo para la suma, en la operaci贸n `a + a` tenemos:\n",
        "```python\n",
        "def __add__(self, other):\n",
        "    out = Value(self.data + other.data, (self,other), '+')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad = 1.0 * out.grad\n",
        "      other.grad = 1.0 * out.grad\n",
        "```\n",
        "\n",
        "Aqu铆 `self` y `other` son en realidad el mismo nodo.\n",
        "Esto significa que primero asignamos `self.grad = out.grad`, y luego lo **sobrescribimos** con `other.grad = out.grad`.\n",
        "En vez de obtener 2, nos quedamos solo con 1.\n",
        "\n",
        "Este mismo problema aparece en cualquier operaci贸n donde el mismo nodo se use m谩s de una vez, por ejemplo en multiplicaciones como `a * a`.\n",
        "\n",
        "**驴C贸mo lo solucionamos?**\n",
        "\n",
        "La clave est谩 en que los gradientes no deben sobrescribirse, sino acumularse.\n",
        "En backpropagation, un nodo puede recibir contribuciones de m煤ltiples caminos del grafo, y todas ellas deben sumarse.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a7BRplC0So8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nueva implementaci贸n de la clase value\n",
        "class Value:\n",
        "  def __init__(self, data, _children=(), _op='', label=''):\n",
        "    self.data = data\n",
        "    self.grad = 0\n",
        "    self._backward = lambda: None\n",
        "    self._prev = set(_children)\n",
        "    self._op = _op\n",
        "    self.label = label\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"Value(data={self.data},label={self.label})\"\n",
        "\n",
        "  def __add__(self, other):\n",
        "    out = Value(self.data + other.data, (self,other), '+')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += 1.0 * out.grad # Acumular el gradiente\n",
        "      other.grad += 1.0 * out.grad # Acumular el gradiente\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    out = Value(self.data * other.data, (self, other), '*')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += other.data * out.grad # Acumular el gradiente\n",
        "      other.grad += self.data * out.grad # Acumular el gradiente\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def backward(self):\n",
        "    topo = []\n",
        "    visited = set()\n",
        "\n",
        "    def build_topo(v):\n",
        "      if v not in visited:\n",
        "        visited.add(v)\n",
        "        for child in v._prev:\n",
        "          build_topo(child)\n",
        "        topo.append(v)\n",
        "\n",
        "    build_topo(self)\n",
        "\n",
        "    self.grad = 1\n",
        "    for node in reversed(topo):\n",
        "      node._backward()"
      ],
      "metadata": {
        "id": "Nan9zvwJT4zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = Value(3.0, label='a')\n",
        "b = a+a; b.label = 'b'\n",
        "b.backward()\n",
        "draw_dot(b)"
      ],
      "metadata": {
        "id": "ybwThd55UAcD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}